\cleardoublepage
\pagestyle{scrheadings}
\cleardoublepage
\chapter{Theory \& Practice}\label{ch:theory}

This section aims to lay the groundwork and context in which the applied example, a semantic data cube implemented for north-western Syria, was conceived, clarifying its intended purpose. Various relevant terms are explained, if not defined, and broader, on-going global initiatives are described, for which this work hopes to be relevant.

# Framing Terms

Before digging into anything further, a few terms and concepts need to be clarified based on intended use throughout this thesis. Some of these terms are still evolving, so the baseline for their usage needs to be established, but it is by no means definitive.

## Open Data \label{sec:opendata}

\graffito{One definition}
The \acf{OKF} defines knowledge as being *open* when anyone is able to freely access, use, modify, and share it [@openknowledgefoundationOpenDefinitionOpen; @molloyOpenKnowledgeFoundation2011]. This definition of openness is also applied to data, as long as they have an open license, are provided in a machine-readable way including necessary metadata, are user-friendly and provided in an open format. Simply publishing data on the Web and making them available at no financial cost is not sufficient for them to be considered open.

"Open data" as a term does not merely refer to data that are free for anyone to access, use, modify and share, but also encompasses a philosophy for predominantly public organisations in our increasingly knowledge- and service-based global economy [@hossainStateoftheartOpenData2016]. Data that are produced by publicly funded institutions or initiatives ought to be made available at no additional cost to facilitate greater returns from what is a public investment [@janssenBenefitsAdoptionBarriers2012]. This, however, only applies to data that cannot be traced back to specific individuals. Opening data to the public can increase accountability, reproducibility and transparency of the research and decisions made based on them and foster innovation in domains where a lack of data was previously a limiting factor. This assumes that the data are, however, actively being used towards increasing these factors.

There is no intrinsic value in open data, rather the value of open data is created by using them [@janssenBenefitsAdoptionBarriers2012]. Public institutions create and collect a lot of data. Opening data sources to the public that are not linked to individuals has the potential to increase benefits for society, the economy and environment. For example, \ac{EO} images from the publicly funded Landsat mission have no intrinsic value, but a drastic increase in their use is precisely what happened when the archive was opened, leading to much research, information extraction, innovation and applications relevant to many domains [@wulderOpeningArchiveHow2012].

The Copernicus programme offers a plethora of free, full and open data, but the openness of these data can be challenged if they are not being used, which is why the \ac{EC} has established many programs to utilise the data and encourage user uptake. Efforts include the development of operational services for monitoring the atmosphere (\acs{CAMS}), marine environment (\acs{CMEMS}), land (\acs{CLMS}), climate change (\acs{C3S}), emergency management (\acs{EMS}) and security, as well as programmes geared towards users and scalable computing environments, such as \ac{RUS} [@copernicusResearchUserSupport2017] and \ac{DIAS} [@copernicusUpcomingCopernicusData2017]. The more that Copernicus' open data, or any data, is not only being downloaded, but used, the higher the value and greater the benefit, hopefully for all.


## Big Earth Data

Big data is an abstract, dynamic concept that poses new challenges to researchers, and is defined by various unique qualities that describe the "bigness" of big data in comparison to more traditional data sources. Typically, big data refers to a massive amount of unstructured data that cannot be handled, stored or processed by traditional IT, software or hardware within a reasonable time or scope, generally requiring analysis in (near-)real-time [@chenBigDataSurvey2014]. These qualities are discussed in literature often using any number of "V" terms, depending on the source. What is considered big data will differ between application domains and will change over time as technology advances and storage, management, processing and analysis methods and strategies are developed to better handle complex, big data sources.

\graffito{Vs of big data}

The 3 V's, *volume, velocity* and *variety*, were first mentioned by @laney3DDataManagement2001 and later applied to characterising big data. Additional Vs have been used to describe big data over the years, including *veracity, variability, value*, and *visualisation* [@gandomiHypeBigData2015; @nativiBigDataChallenges2015].

\spacedlowsmallcaps{Volume} relates not only to the overall space that data take up on hardware -- size and scale -- but also the notion that the more data that is collected, the less valuable each individual observation becomes. Inferential statistics were intended to make inferences about a larger population of phenomenon based on sample observations, whereas big data sources generally encompass significantly more than a random sample of what they represent [@gandomiHypeBigData2015]. There are limited methods that can utilise so much data in a meaningful way, while also avoiding redundancy and the threat of spurious correlations.

\spacedlowsmallcaps{Velocity} speaks to the rate at which data is generated -- timeliness -- in that data generation outpaces the speed at which data can be managed from acquisition to storage and utilised in a meaningful way. Big data are in motion (e.g. constant stream) and dynamic, therefore require high-velocity capture, storage, discovery and analysis [@chenBigDataSurvey2014].

\spacedlowsmallcaps{Variety} has to do with the numerous formats and structures, differing semantics and many dimensions that data can take. This includes data originating from multiple, heterogeneous sources. Different types of data require different handling thus may apply to different definitions of being big.

\spacedlowsmallcaps{Veracity} of data addresses the aspect of uncertainty and unreliability that accompanies unstructured, often untrusted and messy data sources.

\spacedlowsmallcaps{Variability} and was introduced by \acs{SAS} as an additional dimension of big data [@gandomiHypeBigData2015], and refers to dynamic generation of data, meaning inconsistent flow with peaks and lulls.

\spacedlowsmallcaps{Value} references the potential information that can be extracted from big data, transforming data to information for decision-making.

\spacedlowsmallcaps{Visualisation} speaks to the difficulty of displaying results from huge, heterogeneous datasets in a way that can be informative and quick to understand. This is particularly relevant when it comes to multi-dimensional data with a geospatial component.

\graffito{Big Earth Data}

Big data related to Earth sciences, including data about the Earth's atmosphere, surface and interior, are considered big Earth data, but they are more than just \ac{EO} data. Big Earth data as a concept stems from integrating the concept of big data and the vision of digital Earth proposed by @goreDigitalEarthUnderstanding1998. @guoBigDataDrives2017 characterises big Earth data as being "massive, multi-source, heterogeneous, multi-temporal, multi-scalar, highly dimensional, highly complex, non-stationary and unstructured." Observations are also generally not repeatable, depending on the object or process being observed.

As of 7 November 2017, there are an estimated 1,738 active, operating satellites are in orbit with around 25% dedicated to Earth observation, regardless if government, military, commercial or civilian [@UCSSatelliteDatabase2017]. Up until now, the majority of \ac{EO} satellites have been operated by national governments, but the expected future trend is an increase in civilian and commercial satellites [@mccabeFutureEarthObservation2017].

The Landsat-1 mission, launched in 1972, carried the first digital multi-spectral sensors intended for civilian use. Big \ac{EO} data is a subset of big Earth data that really came into existence through the \acs{USGS}'s decision to open the Landsat archive and all new Landsat data to the public in 2008 under a free and open policy [@wulderOpeningArchiveHow2012]. This presented new opportunities for many researchers to access and use Landsat data, stretching back until 1972. Before the archive was opened, a mere 53 Landsat scenes were downloaded on average per day, but now daily averages are around 5,700 scenes [@andersonEarthObservationService2017].

![Estimates of annual data volumes from open and free data from Landsat, \acs{MODIS} (Terra and Aqua units) and the first three Sentinel missions [@soilleVersatileDataintensiveComputing2018] \label{fig:dataestimates}](source/figures/soille_data_estimates_2018.jpg){ width=80% }

The \acp{EC} Copernicus programme has decided to follow a similar free and open data policy and is now providing \ac{EO} data comprised of multiple Sentinel satellite missions operated by \ac{ESA} and in situ observations with global coverage. These data are being provided at an unprecedented frequency and spatial resolution for free, full and open data. There has never been so much \ac{EO} data freely and openly available to the public. Daily data volumes from the Sentinel missions alone are expected to exceed 10\acs{TB} (\autoref{fig:dataestimates}) [@soilleVersatileDataintensiveComputing2018].

\graffito{Need for automated methods}

Remotely-sensed big Earth data could continue to be analysed using methods that were applied to remotely-sensed images before Landsat opened its archive in 2008 (i.e. before they became a big open data source) but most \ac{EO} data available would never be accessed, much less used. This means moving beyond file-based analysis, where each analyst must download each scene one by one, and minimising the transfer of data between data archives/stores and users.

The sheer amount of data is the main challenge in other kinds of big data domains, but satellite data needs conversion into value-added information to harness their potential. Many other spatial analysis routines can be chained together and automated to a large degree, yet analysis of \ac{EO} images often requires more user interaction to produce meaningful results. \ac{EO} images are considered unstructured data, since they generally lack necessary structural organisation for machine-readability and automated analysis. They demand new and different automated methods to leverage their potential, whether related to storage, processing, access or analysis -- algorithms that can handle many images, multiple sensors and many geographical areas and moments in time with heterogeneous conditions (e.g. cloud cover, climate, land cover). Developing large-scale, automated (i.e. repeatable and reliable) methods for extracting information from huge amounts of \ac{EO} data is not only the current trend, but the only foreseeable way to derive meaning from a rapidly growing, global data source.

\graffito{Big Earth data is basically destined to serve international initiatives!}

The benefits of big Earth data transcend the domain of Earth sciences @guoBigDataDrives2017. Free and open \ac{EO} data are the only sources that can continuously provide consistent information about the current and past state of the Earth's surface with global coverage that is, theoretically, the same everywhere. However, the quality and fitness of data for analysis may be unevenly distributed and differ depending on sensor and the geo-spatio-temporal location. Nevertheless, information extracted from big Earth data is a promising source of spatially-explicit evidence.


## Analysis Ready Data \label{sec:ARD}

There has been a lot of hype recently around the term \acf{ARD}, but it lacks a common, widespread definition. \ac{ARD} has mostly been used in the realm of satellite imagery and seems to be related to concepts of openness (see \ref{sec:opendata}), in that the intention is to make data more user-friendly, removing necessary pre-processing steps [@holmesAnalysisReadyData2018]. Data preparation can include re-projection, organisation of data into a regular grid, co-registration, atmospheric correction, various kinds of calibration (e.g. \ac{ToA}, \ac{BoA}), cloud detection or masking, image stacking to create a time-series for analysis, and more. However, data that is "analysis-ready" depends on the intended analysis, and can mean different things to different people, depending on the purpose or application.

Providers of satellite imagery are much better equipped to conduct routine pre-processing than most users. Until recently, satellite data was pre-processed by users, which often led to different pre-processing results even before analysis was conducted. These differences in initial data for analysis, even when using the same fundamental datasets, has made comparability and reproducibility of analysis and results challenging.

Removing the user from pre-processing removes many design decisions making data-use simpler, but obscures the assumptions made by routine processes that may have an impact on analysis results. Many of these pre-processing algorithms, including those used to process open \ac{EO} data, are proprietary and closed to scientists, providing a barrier to understanding the data being analysed. Even if it is convenient to have data pre-processed to a certain level and may improve comparability of analysis conducted by different people, as long as closed algorithms are used, scientific research will be negatively affected.

\graffito{Initiatives providing their definition of ARD}

Data provided under the name \ac{ARD} seems to attempt to close the gaps between data availability, access and use, reducing the processing burden on users. Some current examples of \ac{EO} data currently being provided as \ac{ARD} include products provided by the \ac{USGS}, \ac{AGDC} and \ac{SDC}. The \ac{AGDC}, whose technology has evolved into the \ac{ODC}, provides Landsat data calibrated to surface reflectance and in regular grids projected to suit Australia [@lewisAustralianGeoscienceData2017]. Using similar technology now known as the \ac{ODC}, the \ac{SDC} is offering Landsat \ac{ARD} for Switzerland [@giulianiBuildingEarthObservations2017]. Landsat \ac{ARD} are provided by the \ac{USGS} for the conterminous United States in a common tiling scheme different from the traditional path and row system of the Landsat mission [@usgsLandsatAnalysisReady2018]. Multiple pre-processing levels and products are provided by the \ac{USGS}, including data calibrated to \ac{ToA} reflectance, \ac{ToA} brightness temperature, surface reflectance (i.e. atmospherically corrected), and a sort of pixel quality assessment.

\graffito{Semantic enrichment, information, on-demand access...}

While \ac{ARD} may not yet have a cohesive definition, in the near future, it is likely to encompass much more. Some suggestions include: cross-sensor and cross-provider standardisation to improve use of various data sources in analysis; improved unusable data masks and data quality metrics; methods to take into account that different sensors use different atmospheric models for atmospheric correction; on-demand, user-defined data selection, projection and gridding; semantic content-based image retrieval; user-defined composites and mosaics; and cross-sensor alignment [@holmesAnalysisReadyData2018; @holmesOnDemandAnalysisReady2018]. The term might include not only data, but also automated and routinely generated information products that can be used as inputs for further analysis, monitoring, or even on-demand Web-based online processing. These could include information layers or products such as land cover, land cover change, burned areas, dynamic surface water extents and fractional snow covered areas [@usgsLandsatAnalysisReady2018]. Provision of cloud processing for users to conduct their own algorithms on data predominantly considered analysis-ready exist or are being developed, such as \ac{GEE} [@gorelickGoogleEarthEngine2017] and at least three Copernicus \acp{DIAS} [@copernicusUpcomingCopernicusData2017].


## Semantic Enrichment \label{sec:semantic_enrichment}

Applied to satellite imagery, semantic enrichment is about transforming pixel values into meaningful information.  Initial, generic semantic information is a prerequisite for content-based image and information retrieval and semantic queries, and enables comparison of different images based on their content and content-related fitness for given analysis rather than relying on general characteristics already provided in the metadata (e.g. acquisition time, geographic extent) [@tiedeArchitecturePrototypicalImplementation2017].

\graffito{automatic knowledge-based spectral categorisation}

One existing transferable method for initial, generic semantic enrichment is automatic knowledge-based spectral categorisation, otherwise known as preliminary classification. which increases automation of \ac{EO}-based indicator extraction. Image pre-classification is an initial classification of remotely-sensed images for use in image understanding workflows. According to @marrVisionComputationalInvestigation1982, human vision begins with a pre-attentive first stage. The output of this pre-attentive first stage is a symbolic primal sketch, including both a raw and final version. The raw primal sketch is made of pure spectral differentiation of grey shades and colour tones, and the final primal sketch groups similar shades and tones. Pre-classification is a primal sketch as defined by Marr, where semi-concepts are groups of spectrally similar pixels.

Pre-classification is the assignment of each pixel to a spectral category using *a priori* knowledge-based spectral rules (i.e. a physical model) according to the pixel's spectral signature. Semi-concepts are considered semi-symbolic in that they are an initial step in connecting sensory data (i.e. pixel values) to symbolic, semantic classes equal or inferior to land cover classes, which reduces the dimensionality of data in a repeatable way [@baraldiOperationalAutomaticRemote2012a]. They require further context, analysis, or additional information to classify pixels into symbolic classes, such as land cover classes. For example, the spectral category AVHNIR stands for *Average Vegetation with High Near-Infrared* values, which could be a mixed forest, vegetated cropland, an urban park or something else entirely. More general to more detailed semi-concepts may be considered a sort of multi-scale segmentation [@baraldiOperationalAutomaticRemote2012a].

Applying generic, semantic enrichment moves away from application-based algorithms (e.g. water classifiers) and sample-based classifiers, which are often not transferable among multiple images at different spatio-temporal locations. Completely automated remotely-sensed image understanding is something for the future, but pre-classification can be understood as a first, fully automated step towards image understanding, which is envisioned to include land cover classification [@baraldiOperationalAutomaticRemote2012]. Automatically generated semantic enrichment transforms \ac{EO} images into meaningful information in an automated way.


## Data Cube \label{sec:datacube}

The data cube concept and technologies were developed independent of the \ac{EO} domain [@nativiViewbasedModelDatacube2017]. Also known as data hyper-cubes, they were originally designed to analyse statistics and business data using \ac{OLAP} [@stroblsix].

Data cubes have recently been gaining more traction, especially in the \ac{EO} domain, in order to provide access to \ac{ARD}, however defined, and enable multi-dimensional analysis. Advances in cloud processing environments expand many opportunities for multi-dimensional analysis using data cube infrastructures, which is one way to bring users directly to data, rather than data to users.

Exactly what constitutes a data cube and related standards are currently under development. A manifesto was created by @baumannDatacubeManifesto2017 in response to recent attention in the realm of big Earth data in order to clarify principles of data cube service requirements. They recognise that axes all need to be treated alike, irrespective of having spatial, temporal or other semantics, and that extraction, processing, filtering and fusion must be possible in an ad-hoc way. A data cube as defined by @baumannDatacubeManifesto2017 is:

> a massive multi-dimensional array, also called "raster data" or "gridded data"; "massive" entails that we talk about sizes significantly beyond the main memory resources of the server hardware. Data values, all of the same data type, sit at grid points as defined by the *d* axes of the *d*-dimensional datacube. Coordinates along these axes allow addressing data values unambiguously.

\graffito{data cubes support ARD}

@baumannDatacubeManifesto2017 claim that by following their identified requirements, massive gridded spatio-temporal data stored in data cubes becomes analysis-ready. In the case of \ac{EO} data, this also requires necessary pre-processing, as mentioned in section \ref{sec:ARD}, but data cubes might be increasingly common in the future of \ac{EO} data storage, access and analysis.


## Reproducibility

Reproducibility is one of the fundamental meta-concepts in science, the basis of the "scientific method". It is not just about reproducing an experiment or a result, but also increasing the transparency of the process leading to its outcome. Scientific reproducibility in \ac{EO} data analytics has not been comprehensively studied.

Across various scientific disciplines and in everyday language, there has long been confusion about what reproducibility means as well as varying definitions. @plesserReproducibilityVsReplicability2018 and @nustReproducibleResearchGIScience recently compared a few definitions from different domains. In the context of \ac{EO} data analytics, the following terms are understood in the context of this thesis:

\spacedlowsmallcaps{replicability}: ability for a *different team* to obtain the *same results* through an array of *different methods* using *independently collected data*

\spacedlowsmallcaps{reproducibility}: ability for a *different team* to obtain the *same results* through the *same methods* using the *same data*

\spacedlowsmallcaps{repeatability}: ability for the *same team* to obtain the *same results* through the *same methods* using the *same data*

\spacedlowsmallcaps{transferability}: ability for *any team* to obtain *comparable results* through the *same methods* using *different data*

@goodmanWhatDoesResearch2016 clarified reproducibility by breaking down different aspects within research. Space and time are fundamental to \ac{EO}, since every spatio-temporal location of an observation is unique and cannot be repeated. This differs from some other scientific disciplines, where data to test a hypothesis can generally be assumed to be independently collected at any time or place given the necessary materials and knowledge (e.g. testing the law of gravity). Listed below is how specific kinds of reproducibility in \ac{EO} data analytics based on @goodmanWhatDoesResearch2016 are understood in this work, recognising that independent \ac{EO} data collection to test hypotheses is often not possible (e.g. historical land cover change detection), depending on the spatio-temporal location, scale and context of the events, processes, states, objects, etc. in question:

\graffito{different dimensions of reproducibility}

\spacedlowsmallcaps{methods reproducibility}: ability to exactly repeat the *same methods* by providing sufficient detail about procedures and data

\spacedlowsmallcaps{results reproducibility}: ability to obtain the *same results* through the *same methods* using the *same data*

\spacedlowsmallcaps{inferential reproducibility}: ability to draw qualitatively *similar conclusions* from results obtained by using *different methods*, *different or independently collected data* or by reproducing the original study


## United Nations' Sustainable Development Goals

Many goals have been identified in the scope of various international agreements, with the expressed purpose of improving the lives of people across the world and mitigating potential or inevitable risks and vulnerabilities. Multiple targets have been identified for each of these goals. In this context, indicators exist or are being developed in order to monitor targets and report on progress over time using increasingly standardised and reproducible methods. Many of the developed indicators are based on official statistics at a regional, national or provincial level, but are not necessarily spatially explicit. In order to keep the scope manageable, the focus here is looking more closely at the \ac{UN} \acp{SDG}, which pertain to goals set in 2015 for 2030.[^1]

[^1]: Other international agreements with identified indicators that \ac{EO} data could serve include the \acf{DRR} [@Resolution692832015], the Paris Agreement on Climate Change, and the New Urban agenda [@corbaneBigEarthData2017].

The \ac{UN}'s 193 Member States have identified and agreed to work towards 17 interconnected goals with many targets and a related indicator framework for the 2030 Agenda for Sustainable Development [@Resolution70Transforming2015]. These goals are known as the \acp{SDG} (\autoref{fig:SDGs}), and replace what were the eight \acp{MDG}, presented by the \ac{UN} to be achieved by 2015. Development is a charged concept (e.g. what does it mean to be "developed" as a country?), with many different connotations. That is beyond the scope of this work and will not be discussed, but any reference to development refers to how it is characterised through the \acp{SDG}.

![United Nations Sustainable Development Goals (Source: <https://www.un.org/sustainabledevelopment/news/communications-material/>) \label{fig:SDGs}](source/figures/E_2018_SDG_Poster_without_UN_emblem_Letter US.png)

The \acp{SDG} are different from the \acp{MDG} not only in their content, but also in how they were conceived and to whom they apply. The \acp{MDG} were specifically and unfairly geared towards poorer countries, developed primarily by stakeholders from the United States, Europe and Japan, and co-sponsored by financially motivated international stakeholders including the \ac{IMF}, World Bank and \ac{OECD} [@fehlingLimitationsMillenniumDevelopment2013]. A distinction between "developed" and "developing" countries was made in the creation and implementation of the \acp{MDG} framework. In contrast, the currently active \acp{SDG} are more expansive, overarching ideals that all countries ought to make steps towards achieving. These goals are interrelated, including mitigating climate change, reducing poverty and hunger, improving gender equality and education.

In order to encourage support towards reaching these goals, concrete targets have been identified to track progress of each goal, resulting in a total of 169 targets [@Resolution713132017]. Multiple, measurable indicators for monitoring progress towards each target have been identified, and are intended to be as standardised as possible.


# EO-based Indicators

Much analysis in the domain of \ac{EO} images was previously limited to manually selected, hopefully representative images for the purpose of a given study. In terms of detecting changes, bi-temporal change analysis was commonly employed, due to the prohibitive cost of images, and limited technological tools and hardware to handle, store and process large data. Big, free and open \ac{EO} data have made access to images no longer a limiting factor, and transferability of existing methods to multiple images at the same geographic location, with different seasonal conditions, different temporal locations, or different geographic locations etc. has become increasingly important.

When it comes to \ac{EO} images, indicator extraction is necessary because the reflectance observed by a sensor is only a proxy for detecting, identifying and monitoring objects, events and processes. Pixels representing similar reflectance values can represent different objects, surfaces, etc. Optical \ac{EO} data does not contain direct measurements of most objects or events on Earth (i.e. mixed pixels or relatively slow events). Non-physical entities (e.g. political boundaries) also cannot be directly measured.

Indicator development is imperative for leveraging the potential of \ac{EO} data and transforming them into meaningful and actionable information. As big, open and free data sources are collected over a longer timespan, standardised indicators transferable to multiple data sources will be increasingly useful for interpreting a variety of \ac{EO} data. It will also be important to develop new methods to appropriately analyse and interpret indicators derived from big \ac{EO} sources, which may differ in assumptions to previously limited, sample-based analysis.

Information derived from \ac{EO} data are based on images that have been somehow semantically enriched or classified, which, even if automated and validated, always includes subjective decisions. A first step is often to reduce the dimensionality of the data in a meaningful way. This can be done by employing various classification techniques, calculating indexes (e.g. greenness index, \ac{NDVI}, \ac{NDWI}, \ac{NDSI}, \ac{NDBI}}), or computing aggregative statistics over the temporal stack.

\ac{EO}-based indicators are categorically different from other statistical measures, such as population counts, disease prevalence rates, \ac{GDP}, etc., where things in focus already have a commonly understood semantic association (e.g. a person or currency) and are less ambiguously delineated. This contrasts to some concepts observed in \ac{EO} images, such as something as simple as a forest, where definitions vary depending on location, policy, language and culture. \ac{EO}-based indicators can serve as spatially explicit evidence to support other already identified indicators, and should be interpreted with care when used in isolation.

As @mainiSendaiFrameworkDisaster2017 points out, indicators have potential to be misleading if the data, assumptions or analyses that they are based on are biased or incorrect. In the case of global free and open \ac{EO} data, this applies less to the data, and more to the transformation into information, analysis and interpretation. Aggregated data of water observations over time, for example, may mask specific events of high or low water (e.g. floods or droughts), or the ephemerality of some water bodies that do not exist all of the time. Depending on the purpose of analysis, these events may or may not be relevant. It is extremely important to be critical of the assumptions made when transforming \ac{EO} data into information, and the analysis and interpretation that follows. Interdisciplinary teams with knowledge of and experience in the places and domains being assessed will be required to keep these assumptions in check and in perspective, especially in service of global initiatives.

The cause of visible or detected changes cannot be derived from \ac{EO}-based information or images, but paired with additional information about the area, the information can serve as a spatially-explicit indicator to assess the overall situation. For example, a reduction in the area of vegetation could be determined by comparing \ac{EO} data from various years. However, other data and knowledge is necessary in order to understand why. The change could have been caused by an ongoing drought, changes in policy, damage to irrigation infrastructure, a seed shortage, re-location of people due to a conflict, or other drivers of land-use change.

Satellite-based \ac{EO} has access to any area of the world, and since free and open \ac{EO} data are independent of political boundaries, if not global in coverage, indicators derived from them will be especially useful in supporting international initiatives and agreements in various thematic domains. Utilising information based on free and open \ac{EO} data can reduce costs for monitoring some of an initiative's goals in a consistent and standardised way independent of existing national borders. They can offer dynamic, regionalised information that can be generated more frequently than traditional statistical survey methods. This is true assuming the \ac{EO} data have sufficient quality, are suitable for the intended purpose or indicator, and are generated using automated information extraction methods. Replicable extraction of generic \ac{EO}-based indicators can complement indicators and information from other in-situ sources as evidence for consilience to support decision-makers. Incorporating information derived from an objective base of constantly and continuously collected \ac{EO} data with existing indicators can offer spatially explicit, reliable and unbiased evidence in a timely manner to better facilitate and inform action and monitor progress.

Information alone will not enact changes, nor further progress towards achieving goals like the \acp{SDG}. People *do* those things and through a complex nexus of collaboration over time. However, timely, meaningful information from a common, unbiased, free and open global data source can support and inform those taking action and writing policy in ways previously not possible. Hopefully the generation, but more importantly the use, of \ac{EO}-based information will increase the transparency of decision-making processes, lead to better stewardship of where, what and when to focus energy and resources on, and improve accountability of those invested in making change.


## SDG Indicators and EO

The concept of having global goals with accompanying indicator-based approaches to support efforts was first proposed by Guatemalan and Columbian governments and introduced at the Rio+20 Earth Summit (\ac{UN} Conference on Sustainable Development) in 2012 [@hakSustainableDevelopmentGoals2016]. In @Resolution662882012 (p. 47), it was stated that:

>We recognize that progress towards the achievement of the goals needs to be assessed and accompanied by targets and indicators, while taking into account different national circumstances, capacities and levels of development.

A multitude of quantitative indicators for measuring, monitoring and assessing sustainable development have been created over the years, including various indices, indicator sets, integrated, aggregated and composite indicators, and more. However, development, sustainability and well-being are complex topics with a lack of consensus on how to define, much less measure them. While not perfect, the \acp{SDG} provide an internationally accepted policy framework to work within that offers a more comprehensive understanding of development than simply measuring economic phenomena (e.g \ac{GDP}), taking multiple factors into account. The \ac{UN} and other governments and organisations will use the \acp{SDG} in order to shape and inform their policies and agenda until 2030.

Indicators are important tools for communicating a problem, making it visible, and sensitising the public and decision-makers to something that needs change [@janouskovaGlobalSDGsAssessments2018]. @hakSustainableDevelopmentGoals2016 identified the need for relevant indicators to support the \acp{SDG} based on more intensive conceptual and methodological work instead of simply producing new statistics. The \acp{SDG} and targets are part of a policy framework, but indicators are how these ideal, policy parts are actually assessed.

A collection of standardised, quantitative indicators for each of the identified targets has been produced, but exactly how these indicators will be measured, especially in a universal, standardised and operational way, is still a point of discussion. @janouskovaGlobalSDGsAssessments2018 even claim that the existing indicators were developed to support evidence-based policy making, largely driven by data availability rather than conceptual or methodological reasons, without a clear consensus on how to define and operationalise sustainable development. They claim that relying on this set of indicators may lead to distortions in policy if used as the basis for decisions, since they are not a conceptually strong basis for communicating a comprehensive understanding of global progress towards sustainable development. There are over 243 unique indicators for \ac{SDG} targets proposed, which make it difficult to produce a meaningful overview of the status of the \acp{SDG}, much less operationalise all of the indicators in a meaningful way. That aside, challenging or assessing validity of the developed \ac{SDG} indicators is beyond the scope of this work, but it is important to note that there are critiques about their overall utility.

![\acs{SDG} targets and indicators that can be supported by \acs{EO} data according to the \acf{GEO} (Source: @grouponearthobservationsEarthObservationsGeospatial2017) \label{fig:EOSDGs}](source/figures/201704_geo_unggim_4pager-4.png)

Some have decided to turn to free and open \ac{EO} data to realise regular and timely information for some indicators. In the last few years, multiple initiatives have surfaced evaluating the potential for free and open \ac{EO} data to support monitoring and information generation for the \acp{SDG}. These initiatives should not come as a surprise, since, as @andersonEarthObservationService2017 points out, Article 76 of @Resolution70Transforming2015 (p. 32) explicitly states:

>We will promote transparent and accountable scaling-up of appropriate public-private cooperation to exploit the contribution to be made by a wide range of data, including earth observation and geospatial information, while ensuring national ownership in supporting and tracking progress.

The \acf{GEO} is explicitly linked to efforts towards sustainable development. It was first described at the 2002 World summit on the Sustainable Development Implementation Plan, and launched in 2005 [@andersonEarthObservationService2017]. It's purpose is to coordinate observations globally that have to do with the state of the Earth, but also has a strong focus on how \ac{EO} and geospatial information can serve international agreements and initiatives.

In 2015, \ac{GEO} launched the initiative \ac{EO4SD}. More generally, \ac{UN-GGIM} and \ac{GEO} work closely with the statistical community and have evaluated the \acp{SDG} targets and indicator framework summarised in \autoref{fig:EOSDGs}.  The identified targets in \autoref{fig:EOSDGs} can benefit from \ac{EO}-derived information towards progress, but not necessarily provide an indicator, whereas the identified indicators can be served by a direct measure or indirect support from \ac{EO}. Having reviewed the goals, targets and indicators previously to accessing the results of this \ac{GEO} evaluation, it seems to be quite comprehensive. It is also clear that \ac{EO}-derived information can in some way contribute to assist decision-makers, nations, organisations and other stakeholders to better plan, monitor targets and track progress towards achieving nearly each and every one of the \acp{SDG} [@committeeonearthobservationsatellitesSatelliteEarthObservations2018].



# State-of-the-Art

\ac{EO} data are highly complex, are rapidly increasing in volume and variety, and are unfortunately underutilised in terms of extracting their information potential. A lack of data can no longer be considered a limiting factor to generating meaningful information, rather a lack of reproducible, reliable and transferable methods. There have been many strides in technology, methods and analysis towards better utilising the potential of free and open big \ac{EO} data. They can offer long time-series with continuity, consistency and comparability that can be utilised complementarily with other existing, more traditional statistical methods.


## Free and open \ac{EO}-derived Information as Evidence

Automated, repeatable and reliable \ac{EO} information extraction can theoretically be conducted over large areas and time-spans (assuming adequate hardware), but requires a combination of deductive (i.e. rule/expert-based) and inductive methods that are multi-dimensional and robust to redundant information. Many challenges exist beyond automatisation and improving processes (e.g. efficiency), including large data volumes, high repetition rates, and identifying significant indicators (e.g. advanced information extraction) for a given purpose. Here are a few examples of transforming free and open \ac{EO} images into information that may be relevant for the \acp{SDG}. To keep this scope manageable, the focus will be on Sentinel-2 and Landsat, including some \ac{MODIS} and night-time light data applications, meaning predominantly optical multi-spectral instruments.


### Night-time Light Data

Global \acp{NTL} data show the locations and brightness of light escaping into space. Most of these lights are electric and originate from human settlements, making \ac{NTL} a useful data source for bridging social science and remote sensing. \ac{NTL} data has been used as an indicator for various socio-economic factors, including energy consumption, population distribution, size and growth, urban extent and estimations of \ac{GDP} [@zhangNighttimeLightRemote2015]. They have also been used for crisis-related applications, such as estimating the number of affected or displaced people in the case of a crisis [@corbaneMonitoringSyrianHumanitarian2016] or early damaged area estimation [@kohiyamaEarlyDamagedArea2004]. Most \ac{NTL} studies up to now have been based on a few dates or annual image composites. Further research in this field would include focusing more on temporal dynamics in \ac{NTL}, taking seasonal or hourly changes into consideration to better inform interpretations of results. For example, areas with limited electricity are sometimes limited to certain hours in the day/night or days of the week, which could impact interpretation of results regionally.

For 40 years, \ac{NTL} data was collected using the \acp{DMSP} \ac{OLS}. The \ac{JPSS} from \ac{NASA} and the \ac{NOAA} includes the \ac{VIIRS} meant to replace the \ac{MODIS} and \ac{AVHRR} sensors for tasks such as night-time light analysis, active fire detection and climate change monitoring. Since 2011, \ac{NTL} data are being captured by the \ac{VIIRS} \ac{DNB}. Methods exist for inter-calibrating \ac{DMSP} with \ac{VIIRS} data in order to gain longer time-series of images for detecting changes before \ac{VIIRS} became operational in late 2011. [@liIntercalibrationDMSPOLS2017] inter-calibrated \ac{DMSP}/\ac{OLS} and \ac{VIIRS} night-time light images in order to retrospectively analyse changes that occurred to human settlement areas during the course of the Syrian civil war.  Pre-processing requires removal of background noise and solar or lunar light contamination, cloud cover screening and exclusion of non-electric light sources (e.g. volcanoes, fires) [@elvidgeVIIRSNighttimeLights2017].

@corbaneMonitoringSyrianHumanitarian2016 integrated \ac{NTL} \ac{EO} data from the \ac{VIIRS} with the \ac{JRC}'s \ac{GHSL} to assess the humanitarian impact of the Syrian conflict in a timely manner by estimating the number of people impacted. The \ac{GHSL} built-up layer is based on analysis of Landsat imagery from 1975 until 2013-2014 [@pesaresiOperatingProcedureProduction2016]. This study developed and tested a method to estimate the number of people affected by the Syrian conflict in a timely, consistent and objective manner. \ac{VIIRS} data from January 2013 until December 2015 have a spatial resolution of 750m and were masked using the \ac{GHSL} built-up layer in order to separate city lights from other night-light emissions (e.g. oil and gas wells). Differences in the detected light intensities between each two consecutive months was calculated and used as a proxy for detecting affected or damaged areas. The \ac{GHSL} built-up layer is based on Landsat data. The number of affected people for the identified areas was calculated using the \ac{GHSL} and disaggregated population data, which the \ac{JRC} used to produce depictions of global population distribution and densities in space and time (\acs{GHS-POP}). Estimated numbers of affected people per month were aggregated by administrative region, or governorate, for reporting.


### Landsat, Sentinel and MODIS \label{sec:landsat_sentinel}

Free and open high resolution multi-spectral optical imagery is produced by Sentinel-2 and Landsat 5/7/8 missions. Sentinel’s multi-spectral satellites, 2A and 2B, run as part of the Copernicus programme (formerly known as \ac{GMES}) led by the \ac{EU}, together at full operational capacity have a revisit time of 5 days over equatorial areas and a relatively high spatial resolution (10-60m) with 13 spectral bands. Offering data already calibrated to \ac{ToA} reflectance, with a collective total of around 3.4 \acs{TB} of data daily, the Sentinel-2 satellites are predominantly used to monitor water cover, vegetation, coastal areas, soils, natural disasters and other features of interest for land services. Landsat 5/7/8 are other multi-spectral instruments with relatively lower spatial resolutions, less frequent re-visit times and without pre-processed \ac{ToA} reflectance, that can be exploited in similar ways, especially where historical data are relevant for comparison.

Landsat and Sentinel-2 sensors are limited in the sense that they can only detect features that are visible (e.g. built structures, vegetation, agricultural fields, roads). These data sources can be utilised to monitor security of livelihood assets (e.g. food or water security), land conflicts, post-crisis structural damage assessment, climate change effects, and more, especially when analysed multi-temporally. Due to the spatial resolution of both missions, pixels are generally mixed, which demands the use of indicator-based analysis methods.

These kinds of data are most applicable to geographic locations that experience low average cloud cover either annually or seasonally. Nearly 70% of the Earth is covered in clouds at any given time, and these clouds are not evenly distributed [@kingSpatialTemporalDistribution2013]. One critical issue is that preliminary cloud masking and an accurate detection of haze conditions are required for analysis. In particular, clouds make optical data useless, while areas in the image affected by haze should be radiometrically corrected in order to avoid discarding potential information.  

One valuable set of information products are the automatically generated \acp{GHSL}. Knowing where human settlements are located, their extent and how it changes over time is important for multiple \acp{SDG}. The \acp{GHSL} are based on analysis of Landsat imagery from 1975 until 2013-2014 and incorporate \acs{SRTM} and \acs{ASTER-GDEM} data using a method of supervised classification based on symbolic machine learning [@pesaresiOperatingProcedureProduction2016]. The different layers include a built-up layer (\acs{GHS-BUILT}), a population grid layer (\acs{GHS-POP}) and a global human settlement model (\acs{GHS S-MOD}), which combines the two previous layers in a meaningful way.

As an example of multi-sensor analysis, @corbaneBigEarthData2017 used Sentinel-1 data to generate more up-to-date information on human settlements than available in the first multi-temporal \ac{GHSL} based on Landsat [@pesaresiOperatingProcedureProduction2016]. They used the results from the analysis of Sentinel-1 data to mitigate commission and omission errors produced by @pesaresiOperatingProcedureProduction2016 by applying an \ac{SML} classifier designed for remote sensing big data analytics. The \ac{SML} classifier used training sets derived from existing global land cover products in order to classify built-up areas from Sentinel-1 data for the \acs{GHS-BUILT}.

@lefebvreMonitoringUrbanAreas2016 utilised Sentinel-2 imagery to monitor urban areas using several separate image classifications, which are then fused using what is called the Dempster-Shafer theory. The classification used to detect urban areas is based on all spectral bands and a texture index called PANTEX, which ultimately integrates the spectral information with what could be considered a spatial component (i.e. texture). @lefebvreMonitoringUrbanAreas2016 also showed that Sentinel-2 and Landsat-8 data can be combined in order to improve the geometric accuracy of Landsat-8 classifications, or the repetitiveness and thematic accuracy of Sentinel-2-based analysis.

@mubarekaIdentifyingModellingEnvironmental2010 derived environmental indicators of conflict-induced land-use changes from field data and Landsat scenes in northern Iraq. They identified environmental indicators from \ac{EO} data linked to population vulnerability, such as conversion of agricultural land to grassland, and calculate a risk index.

@broichSpatiallyExplicitLand2015 used a time-series of \ac{MODIS} data to characterise land surface phenology and climate variability for all of Australia. By land surface phenology, they mean a spatio-temporal characterisation of vegetated land surface's different episodes of greening or browning in order to serve multiple applications (e.g. crop monitoring, vegetation condition, resilience to climate variability).

\ac{GEOGLAM} is a global initiative working closely with \ac{CEOS} specifically towards providing information to support efforts towards achieving the \acp{SDG} second goal to reach zero hunger globally. It uses a plethora of satellite-based \ac{EO}, including Sentinel 1-3, Landsat, and \ac{MODIS}, as well as *in situ* data, crop calendars, crop masks, and more, to produce a suite of global information products. One of these includes a crop monitor for early warning, which, for 83 different countries, measures crop conditions to assess a level of food security risk. It is sometimes difficult to discern how products are produced or validated, and the spatio-temporal differences in quality or confidence.

@hansenHighResolutionGlobalMaps2013 and @potapovLastFrontiersWilderness2017 used Landsat data from 2000 until 2012 or 2013, respectively, to map global changes in forest cover. In particular, they focused on forest gain, but more closely on forest loss.

@framptonEvaluatingCapabilitiesSentinel22013 showed that Sentinel-2 is not only useful for detecting the presence of vegetation on land, but also for monitoring vegetation condition, and even estimating canopy chlorophyll content, but further validation is necessary. Sentinel-2 can also be used to measure water quality parameters, such as total suspended particulate matter, and other properties [@liuApplicationSentinelMSI2017].

@muellerWaterObservationsSpace2016 mapped surface water in Austria using 25 years of Landsat imagery using the \ac{AGDC}, software that is now known as the \ac{CEOS} \ac{ODC}, resulting in the product called \ac{WOfS}. It was produced using a water detection algorithm based on a decision tree classifier. An added bonus is that they also utilised a comparison methodology using a logistic regression and the \ac{SRTM} \ac{DSM} to indicate terrain shadow, mask steep slopes, etc., which allowed them to add a level of confidence to the water observations. They were able to compare results to an existing \ac{MODIS} based open water likelihood product valid from 1999 to 2010.

Scientists from the \ac{JRC} collaborated with Google using \ac{GEE} to quantify the extent, changes and various dynamics of global surface water based on Landsat imagery from 1984 until 2015 [@pekelHighresolutionMappingGlobal2016]. Products, accessible via the online Global Surface Water Explorer [@europeancommissionjointresearchcentreGlobalSurfaceWater2016], include maximum extent, percent of water occurrence and change intensity over the time-series, water seasonality by month, annual water recurrence and various types of water transitions between the first and last observation in the time-series. The results calculated in this product are not produced in an automated way, so the most recent results available are from 2015. The results are, however, global.

@tulbureSurfaceWaterExtent2016 calculated surface water extent dynamics in a semi-arid region of Australia based on three decades of seasonally continuous Landsat data using a random forest classifier. They were able to capture ephemeral water, floods, and more "permanent" water bodies, collectively known as surface water flooding dynamics.

In the scope of the \ac{EC} \ac{FP7} project, \ac{G-SEXTANT}, [@tiedeAutomaticPostclassificationLand2014] demonstrated a fully automated, parameter-free post-classification land cover change detection method based on a Landsat time-series. The focus was on changes to agricultural areas in the north-western Syrian-Turkish border region as a potential indicator for livelihood security, conflict-related changes or regional stability in areas where the regional climate requires irrigation to support crops. The information was used as part of a preliminary impact assessment of the Syrian conflict using data from August 2010. 2013 and 2014.

Other studies have used the same prior-knowledge-based classification to detect or characterise changes to land cover. @arvorMonitoringThirtyYears2018 used a 30-year-long time series of pre-classified Landsat images processed in order to monitor small water reservoirs in Brazil. @langerLongtermMonitoringEnvironmental2015 also used pre-classified multi-temporal Landsat scenes from 1994 until 2015 as input to an object-based, post-classification change comparison. This analysis was used to characterise environmental changes occurring around a refugee camp. @hagenlocherEarthObservationbasedApproach2015 used the technology to support semi-automated classification of refugee and \ac{IDP} camps using \ac{VHR} and Landsat data.

@tiedeImageQueryingEarthObservation2016 utilised pre-classification in a parameter free and fully automated workflow that allowed semantic queries as part of a prototypical image understanding system called \acf{IQ}. This system was used by @sudmannsAutomaticExpostFlood2017 to automatically extract surface water in an area in Somalia based on 78 Landsat-8 images. \ac{IQ} is based on a Rasdaman array database architecture with a Web-based \ac{GUI} that allows users to create ad-hoc semantic queries.


## Relevant data cube implementations

Current, existing data cube implementations are geared towards providing access to analysis ready data, and recent developments for \ac{EO} data are numerous. Technical infrastructure implementations for instantiating data cubes employed in the \ac{EO} domain include the Rasdaman array database [@baumannMultidimensionalDatabaseSystem1998], SciDB array database [@stonebrakerSciDBDatabaseManagement2013], free and open software provided by the \ac{CEOS} \ac{ODC} initiative and a data cube infrastructure based on the \ac{JEODPP} [@nativiViewbasedModelDatacube2017].

EarthServer [@baumannBigDataAnalytics2016] is based on Rasdaman and developed by the same people behind Rasdaman and the data cube manifesto referenced in \autoref{sec:datacube}. It serves as an engine for big Earth data analytics of coverage-type datasets using \ac{OGC} big data standards and the Rasdaman query language for searching, filtering and extraction.

The SciDB data managment and analytics system for multi-dimensional array data, primarily designed by the commercial company, Paradigm4, uses an \ac{AQL} for data access and analysis, and is the basis for EarthDB [@planthaberEarthDBScalableAnalysis2012]. It is also the basis from a Brazilian data cube infrastructure by \ac{INPE} for producing land use and land cover classification maps in Brazil [@camaraProc2017Conference2017; @inpeEsensingBigEarth2016]. SciDB is used to store and analysis \ac{EO} data in an ad-hoc way, and offers interfaces to work with the data using R, python and julia.

The \ac{AGDC} [@lewisAustralianGeoscienceData2017], currently implemented in a high performance computing environment [@evansNCIHighPerformance2015], developed the basis for the free and open software that is now known more commonly as the \acf{ODC}. The \ac{CEOS} has set a goal of twenty operational national-scale data cubes by 2022 [@opendatacubeodcOpendatacubeCEOS2017]. The \ac{SDC} [@giulianiBuildingEarthObservations2017] and CDCol [@ariza-porrasCDColGeoscienceData2017] are similar operational implementations of \ac{ODC} software in Switzerland and Columbia, respectively. After accessing the data using a Python \ac{API}, a Python N-dimensional xarray object is returned that meets the N-dimensional user-defined extent, and analysis then can continue in Python, or other languages (e.g. R).

The \ac{JEODPP} of the \ac{EC} has been in development since 2016, and is specifically geared to facilitate analysis of big Earth data in service of global initiatives [@soilleVersatileDataintensiveComputing2018; @nativiViewbasedModelDatacube2017]. It is being used primarily to develop, integrate and analyse a set of indicators based on satellite and *in situ* \ac{EO} data.

Most current data cube implementations intend to provide their definition of \ac{ARD} to users, which often lacks semantic enrichment necessary for turning data into understandable information products in a more automated way, leaving that to users to conduct. One notable framework for monitoring the Earth's surface using a multi-dimensional data cube is \ac{LiMES}, proposed by [@giulianiLiveMonitoringEarth2017] who are involved with the \ac{SDC} [@giulianiBuildingEarthObservations2017]. \ac{LiMES} identified various challenges building a monitoring framework, one of which was turning data into understandable information products.

One existing example combining fully automatic semantic enrichment of \ac{EO} data in 3-dimensional data cube is the \ac{IQ} system described in \autoref{sec:landsat_sentinel}, which uses a Rasdaman array database [@tiedeImageQueryingEarthObservation2016]. This sort of system allows ad-hoc information extraction by combining declarative querying in array databases with access to generic semantic information layers, then known as semantic querying [@sudmannsArrayDatenbankenFurSemantische2016]. The applied example in this thesis applies a different sort of implementation using Sentinel-2 data in some ways similar to \ac{IQ}, but without a sophisticated query language or \ac{GUI}.


# Framing the Applied Example \label{sec:framing}

Given all of this information at the current moment in time, it makes sense to situate the applied proof-of-concept implementation (\autoref{ch:implementation}) within it. In general, big \ac{EO} data are highly complex, increasing in volume and lack efficient processing capabilities and indicators of quality for workflows, methods and results.

This thesis deals with free and open Sentinel-2 data, a big Earth data source, that is provided to users already calibrated to \ac{ToA} reflectance. Automated workflows are necessary for handling the Sentinel-2 mission's expected 3.4\acs{TB} of daily data *volume* [@esaSentinelHighLevel2017]. The data has a relatively high *velocity* due to global coverage on average every five days at the equator, and quite a data *variety* in terms of consistency and quality levels (e.g. cloud coverage) [@soilleVersatileDataintensiveComputing2018].

Incorporating all of the Sentinel-2 data available for an area including information layers generated through pre-classification in an implementation of the \ac{ODC} can be considered as providing the data in an analysis-ready way. Due to the fully automated preparation of data from acquisition to \ac{ODC} ingestion, this implementation can be considered highly repeatable.

Reproducibility was a strong driver behind this implementation. The reproducibility of information extraction in this case is highly linked to the level of automation of information production, which is quite high. The methods and results ought to be reproducible, given access to the same Sentinel-2 data, versions of \ac{SIAM} and \ac{ODC} software, Python computing environments, code and queries.

The applied example presented here utilises similar automated data preparation as demonstrated by @tiedeAutomaticPostclassificationLand2014 and @sudmannsAutomaticExpostFlood2017, including the pre-classification, but transfers it to Sentinel-2 imagery in an implementation of the \ac{ODC}. It is inspired by analysis capabilities of \ac{IQ}, the prototypical implementation described by @tiedeArchitecturePrototypicalImplementation2017 and the rest of the work conducted within the SemEO project at \ac{Z\_GIS}. The SemEO project was funded by the \acf{BMVIT} under the program "ICT of the Future" (contract no: 855467).

Multiple application areas can be covered based on user-generated queries without requiring re-processing the original data. The generic initial semantic enrichment in conjunction with flexible queries through time allows inferring new information layers or higher semantic levels. This aims toward supporting interactive, ad-hoc analysis where users are not required to download any data, rather only the results they would like to keep.

Validation of the pre-classification information layers or information generated by this implementation is not part of this thesis, though comparison to some existing, externally generated information is made to verify the plausibility of results. It is intended to be viewed as a proof-of-concept and a spring-board for further research leading towards more time-sensitive analysis as well as monitoring long-term goals like the \acp{SDG}.

The example results showcased in the scope of this thesis are intended to provide \ac{EO}-based information that might eventually serve some \acp{SDG}, targets and indicators [@Resolution70Transforming2015]. Targets and indicators pertaining to 3 goals are considered. Two of the indicators (2.4.1 and 6.6.1) are labeled as "tier 3", meaning that there is no internationally agreed methodology that exists for calculating them [@committeeonearthobservationsatellitesSatelliteEarthObservations2018]. The last indicator (15.1.1) is considered "tier 1", meaning that established and acceptable methodology exists and data are already widely available:

---

![\acs{SDG} Goal 2 icon (Source: <https://www.un.org/sustainabledevelopment/news/communications-material/>) \label{fig:goal2}](source/figures/SDG_icons/E_INVERTED SDG goals_icons-individual-RGB-02.png)

\spacedlowsmallcaps{Goal 2} end hunger, archive food security and improved nutrition and promote sustainable agriculture

\spacedlowsmallcaps{Target 2.4} by 2030, ensure sustainable food production systems and implement resilient agricultural practices that increase productivity and production, that help maintain ecosystems, that strengthen capacity for adaption to climate change, extreme weather, drought, flooding and other disasters and that progressively improve land and soil quality.

\spacedlowsmallcaps{Indicator 2.4.1} proportion of agricultural area under productive and sustainable agriculture

---

![\acs{SDG} Goal 6 icon (Source: <https://www.un.org/sustainabledevelopment/news/communications-material/>) \label{fig:goal6}](source/figures/SDG_icons/E_INVERTED SDG goals_icons-individual-RGB-06.png)

\spacedlowsmallcaps{Goal 6} to ensure availability and sustainable management of water and sanitation for all.

\spacedlowsmallcaps{Target 6.6} by 2020, protect and restore water-related ecosystems, including mountains, forests, wetlands, rivers, aquifers and lakes.

\spacedlowsmallcaps{Indicator 6.6.1} change in the extent of water-related ecosystems over time

---

![\acs{SDG} Goal 15 icon (Source: <https://www.un.org/sustainabledevelopment/news/communications-material/>) \label{fig:goal15}](source/figures/SDG_icons/E_INVERTED SDG goals_icons-individual-RGB-15.png)

\spacedlowsmallcaps{Goal 15} protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt the reverse land degradation and halt biodiversity loss

\spacedlowsmallcaps{Target 15.1} by 2020, ensure the conservation, restoration and sustainable use of terrestrial and inland freshwater ecosystems and their servces, in particular forests, wetlands, mountains and drylands, in line with obligations under national agreements.

\spacedlowsmallcaps{Indicator 15.1.1} forest area as a proportion of total land area

----------------------------

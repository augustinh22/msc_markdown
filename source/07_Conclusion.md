\cleardoublepage
\pagestyle{scrheadings}
\cleardoublepage
\chapter{Conclusion}\label{ch:conclusion}

- absolute distillation of achievements in max. 2 pages

Extracting information from \ac{EO}-time-series is a challenging task due to the volume, velocity and variety of \ac{EO} images. The largest benefit of the semantic data cube implemented here is that it fully automates data acquisition, semantic enrichment and access to data ready for analysis. The generic, application-independent semantic enrichment utilised here enables queries and \ac{EO}-based indicator extraction for many thematic tasks. Since the information layers (i.e. basis for semantic queries and analysis) continue to exist in the data cube and are stable concepts, given solid documentation on methods applied to data cube output, reproducible results and repeatable analysis ought to be possible.




## GI_Forum extract

Data cubes are on the rise as a viable solution for remotely sensed big EO data storage and analysis. They allow users to access the same pre-processed data, supporting reproducible analysis, and facilitate analysis using dimensions beyond the spatial (e.g. time) as additional axes in the data cube.

The innovation presented here is the set-up of a semantic data cube, which, in contrast to existing data cubes, stores information together with the data, thus allowing ad-hoc semantic queries. This is made possible via a fully automated workflow, including generic semantic enrichment of data to information layers, which adds this functionality to the ODC. Exemplarily shown is utilisation of the semantic data cube for a surface water dynamics extraction in a use case located in Syria. Due to the generic approach, the same pre-processed data can be queried without changes for other EO-based indicator extraction in a wide variety of thematic domains. This avoids application and data specific classification algorithms as commonly proposed in recent open data cube literature. With the availability of free and open spatially and temporally high resolution data, we are expecting a general movement away from bi-temporal change analysis to change through or utilising time rather than controlling for it, as has been seen in EO-data analysis up to now. This implementation enables multi-temporal queries and analysis.

Indicator development based on dense EO time-series, or seasonal slices, is the next step to leveraging the potential of EO data, especially as data sources, such as Sentinel-2, are collected over more than just a few years’ time. Indicator extraction is necessary because the reflectance is only a proxy for detecting and identifying objects. This implementation can assist in detecting, monitoring, quantifying and even discovering new visible land cover dynamics and processes (e.g. meandering rivers, impermanent lakes, irrigated agriculture patterns, uncharacteristic vegetation removal), and offer evidence for supporting the requirements, actions and goals of multiple existing global initiatives.

The exploitation of the value of Big Earth Data involves automation, pre-processing, on-demand querying and compelling visualisation of the results. Massive processing power in the cloud and fast network connection is required, but not sufficient. Automation of intelligent workflows leading to pre-processing of data are important drivers for on-demand and ad-hoc querying to extract information in real time. Semantically enriched data allow also unexperienced users to formulate queries by means a high-level declarative language. Instead of having to translate an algorithm into software code manually, the query will be evaluated by the system and transformed into optimised physical access patterns. This approach can be realised by automatic (application independent) semantic enrichment of \ac{EO} images in Big \ac{EO} image databases, which are therefore “prepared” and “ready” for application specific queries in distributed array databases (with a declarative query language and a query optimiser). This approach avoids redundancy in data handling and repeated data (pre-) processing. The feasibility of this approach has been proven by Tiede et. al. [2016].
